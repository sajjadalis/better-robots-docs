# Settings Guide

Complete guide to all Better Robots.txt settings and configuration options.

## Settings Overview

Better Robots.txt provides comprehensive settings organized into logical sections:

- **Basic Settings**: Core robots.txt configuration
- **Bot Management**: Control which bots can access your site
- **SEO Integration**: Sitemap and search engine optimization
- **Advanced Options**: Custom rules and specialized features
- **E-commerce**: WooCommerce-specific optimizations

## Basic Settings

### 1. Default Rules

**Location**: Settings ‚Üí Basic Configuration

These are the foundational rules applied to all robots.txt files:

```txt
User-agent: *
Allow: /wp-admin/admin-ajax.php
Allow: /*/*.css
Allow: /*/*.js
Disallow: /wp-admin/
Disallow: /wp-includes/
Disallow: /readme.html
Disallow: /license.txt
Disallow: /xmlrpc.php
Disallow: /wp-login.php
Disallow: /wp-register.php
Disallow: */disclaimer/*
Disallow: *?attachment_id=
Disallow: /privacy-policy
```

**What these do**:
- ‚úÖ **Allow**: Essential WordPress functionality and resources
- ‚ùå **Disallow**: Admin areas, sensitive files, and duplicate content

### 2. Custom User Agents

**Location**: Settings ‚Üí Custom Rules

Add your own robots.txt rules using the textarea:

**Format**:
```txt
User-agent: Googlebot
Allow: /important-page/

User-agent: *
Crawl-delay: 1
```

**Common Examples**:
```txt
# Block all bots from temporary directory
User-agent: *
Disallow: /temp/

# Allow Googlebot to access everything
User-agent: Googlebot
Allow: /

# Set crawl delay for all bots
User-agent: *
Crawl-delay: 2
```

### 3. Crawl Delay

**Location**: Settings ‚Üí Performance

**Purpose**: Controls how fast bots can request pages from your site

**Recommended Settings**:
- **Shared hosting**: 1-2 seconds
- **VPS/Dedicated**: 0.5-1 seconds
- **High-traffic sites**: 2-5 seconds

**Format**: `Crawl-delay: 1` (number in seconds)

### 4. Personalization

**Location**: Settings ‚Üí Customization

Add custom text or comments to your robots.txt file:

```txt
# My Custom Website Rules
# Created by: Your Name
# Contact: your-email@example.com

# Special Rules
# Add any comments here
```

## Bot Management Settings

### 1. Major Search Engine Bots

**Location**: Settings ‚Üí Bot Management ‚Üí Search Engines

Control access from major search engines:

| Bot | Default | Recommended | Impact |
|-----|---------|-------------|---------|
| Googlebot | Allow | ‚úÖ Allow | Essential for Google Search |
| Bingbot | Allow | ‚úÖ Allow | Essential for Bing Search |
| Yahoo Slurp | Allow | ‚úÖ Allow | Essential for Yahoo Search |
| DuckDuckBot | Allow | ‚úÖ Allow | Good for privacy-focused users |
| Yandex | Allow | ‚ö†Ô∏è Depends | Only if targeting Russian audience |

### 2. Media Crawlers

**Location**: Settings ‚Üí Bot Management ‚Üí Media

Control image and media indexing:

| Bot | Purpose | Recommendation |
|-----|---------|----------------|
| Googlebot-Image | Google Images | ‚úÖ Allow (if good images) |
| YandexImages | Yandex Images | ‚ö†Ô∏è Depends on audience |
| Msnbot-media | Bing Images | ‚úÖ Allow (usually safe) |

### 3. Specialized Bots

**Location**: Settings ‚Üí Bot Management ‚Üí Specialized

| Bot | Purpose | Recommendation |
|-----|---------|----------------|
| Applebot | Siri & Spotlight | ‚úÖ Allow (increasingly important) |
| Mediapartners-Google | Google Ads | ‚úÖ Allow (if using ads) |
| AdsBot-Google | Ad quality checking | ‚úÖ Allow (if using ads) |

### 4. ChatGPT Bot Blocker (Free Feature)

**Location**: Settings ‚Üí Bot Protection ‚Üí ChatGPT

**Option**: Block ChatGPT Bot from scraping your content

```txt
# Generated rule when enabled:
User-agent: GPTBot
Disallow: /
```

**Why use this**:
- üõ°Ô∏è Protects content from AI training
- üí∞ Prevents content theft
- üéØ Controls data usage

### 5. Bad Bots Blocker (Free & Pro Features)

**Location**: Settings ‚Üí Bot Protection ‚Üí Bad Bots

Blocks known malicious and unwanted bots to protect your site:

**Free Version - AI-Recommended Bad Bots**:
```txt
# Block Bad Bots. AI recommended setting by ChatGPT

User-agent: ia_archiver
Disallow: /

User-agent: archive.org_bot
Disallow: /

User-agent: SiteExplorer
Disallow: /

User-agent: spbot
Disallow: /

User-agent: WBSearchBot
Disallow: /
```

**Pro Version - Extended Bad Bots**:
- All free bad bots plus 40+ additional malicious bots
- Blocks scrapers and content thieves
- Reduces server load
- Improves security

### 6. Backlink Protector (Pro Feature)

**Location**: Settings ‚Üí Bot Protection ‚Üí Backlinks (Pro)

Protect your competitive intelligence by blocking SEO analysis tools:

**Capabilities**:
- Blocks major SEO analysis bots
- Hides your backlink profile from competitors
- Preserves your SEO strategy and link value

**Why use this**:
- üîí Protects competitive intelligence
- üõ°Ô∏è Hides SEO strategy from competitors
- üí∞ Preserves link value

## SEO Integration Settings

### 1. Sitemap Integration (Pro Feature)

**Location**: Settings ‚Üí SEO Integration ‚Üí Sitemaps

Automatically detect and integrate sitemaps from major SEO plugins:

**Supported Sources**:
- ‚úÖ Yoast SEO - Auto-detection available
- ‚úÖ Rank Math - Auto-detection available
- ‚úÖ All in One SEO - Auto-detection available
- ‚úÖ WordPress Native - Basic WordPress sitemaps
- ‚úÖ Custom URLs - Manual configuration for custom sitemaps

**Benefits**:
- Automatic sitemap discovery
- Better search engine indexing
- No manual configuration needed for supported plugins

### 2. Ads.txt & App-ads.txt Support

**Location**: Settings ‚Üí SEO Integration ‚Üí Ads.txt

**Options**:
- **Allow**: Permits crawler access to ads.txt files
- **Disallow**: Blocks access (rarely recommended)
- **Off**: No specific rule

**Generated Rules**:
```txt
# When allowed:
User-agent: *
Allow: /ads.txt
Allow: /app-ads.txt
```

## Advanced Settings

### 1. Physical File Generation (Pro Feature)

**Location**: Settings ‚Üí Advanced ‚Üí Physical File

Creates an actual `robots.txt` file in your website root directory.

**Benefits**:
- ‚úÖ Better compatibility with caching services
- ‚úÖ Improved PageSpeed Insights scores
- ‚úÖ Reduced server load
- ‚úÖ Works with CDN services

**Considerations**:
- ‚ö†Ô∏è Requires file system write permissions
- ‚ö†Ô∏è May conflict with other robots.txt solutions
- ‚ö†Ô∏è Needs manual updates if modified directly

### 2. Image Crawlability (Pro Feature)

**Location**: Settings ‚Üí Advanced ‚Üí Images

Control how search engines index your images:

**Options**:
- **Allow**: All bots can index images (recommended for visual content)
- **Disallow**: Block image indexing (for text-focused sites)
- **Off**: No specific rules (default behavior)

**Use Cases**:
- **Allow**: Image-heavy sites, photographers, e-commerce stores
- **Disallow**: Text-focused sites, bandwidth conservation

### 3. Crawl Budget Optimization (Pro Feature)

**Location**: Settings ‚Üí Advanced ‚Üí Crawl Budget

Prevents crawl traps and optimizes search engine crawling efficiency:

**What it blocks**:
- Search results pages and parameters
- Preview and duplicate content URLs
- Crawler traps that waste crawl budget
- Unnecessary query parameters

**Benefits**:
- Focuses search engine attention on important content
- Improves indexing efficiency
- Reduces server load from unnecessary crawling

## E-commerce Settings (WooCommerce)

### WooCommerce Optimization (Pro Feature)

**Location**: Settings ‚Üí E-commerce ‚Üí WooCommerce

Optimize your WooCommerce store by blocking unnecessary URLs that waste crawl budget:

**What it blocks**:
- Shopping cart and checkout pages
- User account areas
- Product filter and sorting URLs
- Add to cart action URLs
- Admin-only WooCommerce pages

**Benefits**:
- ‚ö° Reduces server load
- üìà Improves crawl efficiency
- üõçÔ∏è Focuses search engines on product pages
- üí∞ Better SEO for important e-commerce content

## Multisite Settings (Pro Feature)

### Network Management

**Location**: Settings ‚Üí Multisite ‚Üí Network Sites

Manage robots.txt across WordPress multisite networks efficiently:

**Options**:
- **Enable Network Management**: Apply rules to all sites
- **Site Selection**: Choose which sites to manage
- **Network Rules**: Apply common rules across sites
- **Network Sitemaps**: Auto-detect sitemaps for all sites

**Benefits**:
- Centralized management of multiple sites
- Consistent robots.txt rules across network
- Individual site override options
- Time-saving bulk configuration

## Social Media Bots (Pro Feature)

### Platform-Specific Control

**Location**: Settings ‚Üí Social Media

Control how social media platforms access and display your content:

**Supported Platforms**:
- **Facebook**: Facebook crawler for rich snippets
- **Twitter**: Twitter bot for card previews
- **LinkedIn**: LinkedIn crawler for content sharing
- **Pinterest**: Pinterest bot for pin creation

**Options per Platform**:
- **Allow**: Full access for rich social media previews
- **Disallow**: Block social media crawling
- **Off**: No specific rules (default behavior)

**Benefits**:
- Better social media preview generation
- Control over social content access
- Improved social sharing experience

## Post/Page Specific Settings

### Manual Exclusion

**Location**: Edit Post/Page ‚Üí Better Robots.txt Meta Box

Control search engine access to individual pages and posts:

**Options**:
- **Exclude from Search Engines**: Adds specific disallow rules for that page
- **Custom URL Pattern**: Specify custom URL patterns to block

**Use Cases**:
- üéØ Thank you pages (after form submissions)
- üéØ Private landing pages
- üéØ Admin-only content
- üéØ Test or development pages
- üéØ Dynamic calendar pages
- üéØ Member-only content

## Best Practice Settings

### Recommended Configuration for Most Sites

```markdown
‚úÖ **Free Version Features**:
- Keep default robots.txt rules
- Add crawl delay: 1 second
- Enable ads.txt allowance
- Allow all major search engines
- Block ChatGPT Bot (AI-recommended bad bots)

‚úÖ **Pro Version Upgrades**:
- Extended bad bot blocking (40+ additional bots)
- Auto-detect sitemaps from SEO plugins
- Crawl budget optimization
- WooCommerce optimization
- Social media bot control
- Physical file generation
- Multisite management
- Backlink protection
```

### Blog/Content Sites

```markdown
‚úÖ Prioritize: Image indexing, social media bots
‚ùå Avoid: Overly restrictive crawling
‚ö†Ô∏è Consider: CDN compatibility
```

### E-commerce Sites

```markdown
‚úÖ Enable: WooCommerce optimization
‚úÖ Consider: Physical file generation
‚úÖ Block: Unnecessary parameter URLs
‚ö†Ô∏è Monitor: Product page indexing
```

### High-Traffic Sites

```markdown
‚úÖ Increase: Crawl delay (2-5 seconds)
‚úÖ Enable: Crawl budget optimization
‚úÖ Consider: Physical file generation
‚úÖ Monitor: Server performance
```

---

**Next**: Explore our [Examples](/examples) for specific use cases and configurations.